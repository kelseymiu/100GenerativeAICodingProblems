# Generative AI Python Coding Benchmark

## **Overview**

This coding benchmark is part of a research project aimed at evaluating Generative AI's capability to produce Python coding problems. It encompasses 100 varied and distinct coding problems, each accompanied by an automated test. The objective of these questions is to offer a comprehensive assessment tool that measures diverse aspects of Generative AI coding abilities.

## **Contents**

1. Data Structures and Algorithms
2. Generative Basics
3. Text/Image Generation
4. Sequence-to-Sequence Problems
5. Ethics and Bias in AI
6. Reinforcement Learning for Generation
7. Evaluation Metrics
8. Attention Mechanisms
9. Data Preparation
10. Interactivity

## **How to Run**

To execute the coding benchmark:

- Ensure you have Python (>=3.6) installed.
- Clone or download the repository containing the assessment Python file.
- Navigate to the directory in the terminal or command prompt.
- Run the Python file using: python assessment_file_name.py


Each test will execute sequentially and will print a success message upon passing. Any failures or errors will halt the execution, allowing for quick identification and debugging.

## **Novelty and Research Objective**

Traditionally, coding problem sets are manually curated, often lacking in diversity, novelty, and sometimes, real-world relevance. With the rapid advancements in Generative AI, we sought to explore how effectively such models can be employed to generate meaningful, diverse, and challenging coding problems. 

### **What's Novel?**

- **Diversity**: Spanning 11 different categories, from data structures to the nuances of AI ethics.
- **Automated Testing**: Each problem comes with its own test, simplifying validation.
- **Complexity**: Problems vary in complexity, offering assessors a broad view of an individual's or model's capabilities.

## **What It Assesses**

This benchmark evaluates:

- **Problem Solving**: Addressing both foundational and advanced programming concepts.
- **Generative AI Application**: Creating tasks that revolve around generative models and their applications.
- **AI Ethics**: Emphasizing the importance of recognizing and rectifying bias and ethical concerns.
- **Interactivity**: Gauging the capability to design interactive models or utilities.
- **AI-specific Topics**: Including reinforcement learning, attention mechanisms, and transfer learning.

## **Areas for Improvement**

While the benchmark offers a diverse range of problems, there are areas that can be expanded:

- **Depth**: Some advanced AI concepts can be explored in greater depth, with a series of questions per topic.
- **Real-world Scenarios**: More problems simulating real-world scenarios or use-cases can offer practical insight.
- **Modular Testing**: Introduce modular testing where one problem's solution aids in solving a subsequent one, replicating real-world development tasks.

## **Concluding Remarks**

This project is an exploration of how Generative AI can revolutionize education and assessment. The novelty lies in leveraging an AI model's capability not just for problem-solving, but for problem generation across a diverse array of topics, ranging from foundational coding to intricate AI nuances. This not only paves the way for a dynamic, ever-evolving assessment tool, but also emphasizes the possibilities of AI-driven educational tools. Feedback, contributions, and insights from the community are encouraged to refine and expand this benchmark further.
